##### #####################################
##### LEGAL ENGLISH - TEXT ANALYTICS ######
#####            TIDYTEXT            ######
##### #####################################

library(tidyverse)
library(tidytext)
library(readtext)
library(magrittr)
library(broom)
library(tokenizers)
library(quanteda)



##### ######################################
##### Let's do the pre-lim stuff..
##### ######################################

### starting with getting the texts... 
legalTexts <- readtext("sources/",
                        docvarsfrom = "filenames",
                        docvarnames = c("Institution", "DocName",
                          "Type", "Country", "Year"),
                        dvsep = "_")


### this is the default stopwords list
default_stopwords <- unnest_tokens(read.csv("default_stopwords.csv",
                      stringsAsFactors = FALSE), word, word) %>%
                      as_tibble()

### ...and our custom stopwords list...
legalStopwords <- unnest_tokens(read.csv("legalEnglish_stopwords.csv",
                    stringsAsFactors = FALSE), word, word) %>%
                    as_tibble()

# ...and combine those with default stopwords...
stopwords_all <- bind_rows(default_stopwords, legalStopwords)


### FYI the document to be used with sink() used to be created here, but has now been moved
### down to in front of the sink command. This is because it opens the connection on creation



##### ################################
##### ################################
##### OK, it's corpus-making time
##### ################################
##### ################################


### We'll create 3 versions in tidy format - all words, no custom stops, & no all stops

## 1st is the corpus in tidy format, with no words removed
legalTidy_all <- legalTexts %>%
  group_by(DocName) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)  # word is the name of the output column

## 2nd is the corpus in tody format, with custom words removed
legalTidy_noCustom <- legalTidy_all %>%
  anti_join(legalStopwords, by = "word")

## 3rd is the corpus in tidy format, with all stops removed (custom + general)
legalTidy <- legalTidy_all %>%
  anti_join(stopwords_all, by = "word")


##### ###########################
##### WORD FREQUENCIES & COUNTS
##### ###########################

## Let's start with a count of the documents in the corpus
legalDocCount <- legalTexts %>%
  count(DocName) %>%
  summarize(total = sum(n))

## then get the top word freqs (no stops) & make it a DF so we can choose how many rows
legalFreqs <- count(legalTidy, word, sort = TRUE) %>%
                ungroup() %>%
                data.frame()

# and with just the custom words removed
legalFreqs_noCustom <- count(legalTidy_noCustom, word, sort = TRUE) %>%
                          ungroup() %>%
                          data.frame()

# and a freq count of words by document ##### CLEAN THIS UP WITH PIPING #####
legalFreqs_bydoc <- legalTidy %>%
  count(DocName, word, sort = TRUE)

legalWordCountByDoc <- legalFreqs_bydoc %>%
                          group_by(DocName) %>%
                          summarize(total = sum(n))

legalFreqs_bydoc2 <- left_join(legalFreqs_bydoc, legalWordCountByDoc)


## we can have a look at specific terms that show up w/ str_detect
## THIS HANGS RSTUDIO, NEED TO FIND OUT WHY
#legalTexts %>%
#  filter(str_detect(text, "ar")) %>%
#  select(text)



##### ###############
##### BIGRAMS, BABY
##### ###############

## let's tokenise the entire corpus into bigrams (no stops)
legalBigrams_all <- legalTexts %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# ok, how about the most common bigrams?
legalBigrams_count <- legalBigrams_all %>%
  count(bigram, sort = TRUE) %>%
  data.frame()

### now we'll put the words into separate columns, so we can filter
legalBigrams_sep <- legalBigrams_all %>%
  separate(bigram, c("word1", "word2"), sep = " ")

## filtering...
legalBigrams_filtered <- legalBigrams_sep %>%
  filter(!word1 %in% legalStopwords$word) %>%
  filter(!word2 %in% legalStopwords$word)

#### NOT USING ATM - ok, now we'll use that to get the most common after filtering
#legalBigrams_filteredcount <- legalBigrams_filtered %>%
#  count(word1, word2, sort = TRUE) %>%
#  data.frame()

### making a new df with just the word columns joined (could use unite(), but smaller this way)
legalBigrams_filteredCount <- paste(legalBigrams_filtered$word1,
                                legalBigrams_filtered$word2, sep = " ") %>%
                              data.frame()

## changing the column name to whatever it says below
colnames(legalBigrams_filteredCount) <- paste("bigrams")

# and finally we'll set it up to return the most common
legalBigrams_filteredCount <- legalBigrams_filteredCount %>%
                                count(bigrams, sort = TRUE) %>%
                                data.frame() # not sure why I need to redo the df, but I do


#### NOT USED ATM -  ok, unite() is the opposite of separate()
#legalBigrams_united <- legalBigrams_filtered %>%
#  unite(bigram, word1, word2, sep = " ")

#### NOT USED ATM - filter bigrams with a particular word
#legalBigrams_filtered %>%
#  filter(word2 == "rop") %>%
#  count(DocName, word1, sort = TRUE)



##### ##################
##### IT's TRIGRAM TIME
##### ##################

## same as bigrams, let's tokenise the entire corpus into trigrams (no stops)
legalTrigrams_all <- legalTexts %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# let's have a look-see at the most common trigrams then
legalTrigrams_count <- legalTrigrams_all %>%
  count(trigram, sort = TRUE) %>%
  data.frame()

### now we'll put the words into separate columns, so we can filter
legalTrigrams_sep <- legalTrigrams_all %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

## filtering...
legalTrigrams_filtered <- legalTrigrams_sep %>%
  filter(!word1 %in% legalStopwords$word) %>%
  filter(!word2 %in% legalStopwords$word) %>%
  filter(!word3 %in% legalStopwords$word)

#### NOT USED ATM - now let's see them common trigrams after filtering
#legalTrigrams_filteredcount <- legalTrigrams_filtered %>%
#  count(word1, word2, word3, sort = TRUE) %>%
#  data.frame()

### making a new df with just the word columns joined (could use unite(), but smaller this way)
legalTrigrams_filteredCount <- paste(legalTrigrams_filtered$word1,
                                legalBigrams_filtered$word2, legalTrigrams_filtered$word3,
                                  sep = " ") %>%
                              data.frame()

## changing the column name to whatever it says below
colnames(legalTrigrams_filteredCount) <- paste("trigrams")

# and finally we'll set it up to return the most common
legalTrigrams_filteredCount <- legalTrigrams_filteredCount %>%
                                count(trigrams, sort = TRUE) %>%
                               data.frame() # gotta check why it went from df to tibble



#### MORE EFFICIENT BUT NOT USING IT AT THE MOMENT
#legalTrigrams <- legalTexts %>%
#  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
#  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
#  filter(!word1 %in% stopwords_all$word,
#          !word2 %in% stopwords_all$word,
#          !word3 %in% stopwords_all$word) %>%
#  count(word1, word2, word3, sort = TRUE)


##### #####################
##### TF-IDF...IDK EITHER
##### #####################

### Let's calculate term frequency rankings (Zipf's Law)
legalZipf <- legalFreqs_bydoc2 %>%
  mutate(rank = row_number(), `term_freq` = n/total) %>%
  arrange(desc(term_freq)) %>%
  data.frame()

### TF-IDF - UNIGRAMS, BIGRAMS, & BEYOND ###

# let's start with the tf-idf for all unigrams
legal_tfidf <- legalFreqs_bydoc2 %>%
  bind_tf_idf(word, DocName, n)


# and look at the unigrams with a high score
# we'll also trim down the document names, as they're too long and wrap the columns!
legal_tfidf_high <- legal_tfidf %>%
  select(-total) %>%
  arrange(desc(tf_idf)) %>%
  data.frame() 
legal_tfidf_high$DocName <- substr(legal_tfidf_high$DocName, 1, 30)


# moving on to bigrams now. Let's start with the top ones
legalBigrams_tfidf <- legalBigrams_united %>%
  count(DocName, bigram) %>%
  bind_tf_idf(bigram, DocName, n) %>%
  arrange(desc(tf_idf))


# ok, now the top trigrams
legalTrigrams_tfidf <- legalTexts %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% legalStopwords$word,
          !word2 %in% legalStopwords$word,
          !word3 %in% legalStopwords$word) %>%
  count(word1, word2, word3, sort = TRUE)


##### #############################
##### KWIC - NOW WE"RE WARMING UP
##### #############################

# First is to convert the texts into a quanteda corpus object
legalCorpus <- corpus(legalTexts)

#### Function to loop through kwic based on highest tf-idf tokens, output to list then bind

legal_kwic_list <- list()
i = 1
while (i <= 10) {
  kwic_results <- kwic(legalCorpus, legal_tfidf_high$word[i], window = 10, valuetype = "fixed")
  kwic_results$i <- i
  
  legal_kwic_list[[i]] <- kwic_results
  i <- i +1
}

legal_kwic_results <- do.call(rbind, legal_kwic_list)

# we can clean up the kwic results for printing by combining the kwic sentences & trim the df...
legal_kwic_resultsOutput <- data.frame(legal_kwic_results)

legal_kwic_resultsOutput$sentences <- paste(legal_kwic_results$pre, legal_kwic_results$keyword,       
                                        legal_kwic_results$post)

legal_kwic_resultsOutput <- legal_kwic_resultsOutput[, -c(2:7)]



##### ###########################
##### OUTPUT TO FILE
##### ###########################

## create the first file for output
legalOutputs <- file("legalEnglish_outputs.txt", open = "w")

# and start the sink
sink(file = legalOutputs, type = "output")
cat("Outputs from analysis of the Legal English Corpus\n\n")
cat("\n\n\n", "Number of documents in the corpus\n\n")
legalDocCount
cat("\n\n\n", "Word Count for the Corpus - no word filtering\n\n",
  "this is the complete word count, and includes abbreviations, symbols, etc\n\n")
count(legalTidy_all)
cat("\n\n\n", "Word Count - custom stop words removed\n",
  "unhelpful words are removed, such as abbreviations, symbols, country names, etc\n\n")
count(legalTidy_noCustom)
cat("\n\n\n", "Word Count - custom and common words removed\n\n",
  "common words are also removed, such as articles, prepositions, and pronouns\n\n")
count(legalTidy)
cat("\n\n\n", "Sum of custom and common words that can be removed\n\n")
count(legalTidy_all) - count(legalTidy)
cat("\n\n\n", "NOTE: Analyses are done with either all stop words removed (custom & common), or\n
  with only the custom removed, depending on what the end goal is. E.g. Top 50 frequencies\n
  have both removed, because we want to see the most important words for the corpus/industry,\n
  while top bigrams, trigrams, and longer language only have custom words removed, because\n
  the 'common' words can play an integral role in those.\n\n")
cat("\n\n\n", "Top 50 Word Frequencies for the corpus (stop words removed)\n\n")
legalFreqs[1:50,]
cat("\n\n\n", "For Comparison: Top 50 Word Frequencies (common words included)\n\n")
legalFreqs_noCustom[1:50,]
cat("\n\n\n", "Top Bigrams (no filtering)\n\n")
legalBigrams_count[1:50,]
cat("\n\n\n", "Top Trigrams (no filtering)\n\n")
legalTrigrams_count[1:50,]
cat("\n\n\n", "Top 50 Highest Scoring Words by tf-idf\n\n
  In the below table:\n
  \t - \t'n'\t is the number of times the word appears in the listed document.\n
  \t - \t'tf'\t is the term frequency score, giving a relative weighting within the document.\n
  \t - \t'idf'\t is the inverse document freq. Relates to how common a word is in a corpus.\n
  \t - \t'tf-idf'\t combines them. High tf with a low idf means the word is more specific.\n\n")
legal_tfidf_high[1:50,]
cat("\n\n\n", "Highest Scoring Bigrams by tf-idf\n\n")
legalBigrams_tfidf
cat("\n\n\n", "Highest Scoring Trigrams by tf-idf\n\n")
legalTrigrams_tfidf
cat("\n\n\n", "Key Words in Context (kwic). 
  \n\n These sentences show the top 10 tf-idf words in context. They are useful in selecting\n
  the words as they are used in context.\n\n")
legal_kwic_results
sink()


## and another just for the kwic sentences
legalOutputs_kwic <- file("LegalEnglish_kwic_sentences.txt, open = w")

sink(file = legalOutputs_kwic, type = "output")
legal_kwic_resultsOutput
sink()


##### ###########################
##### PLOTTING
##### ###########################

#first, let's look at word count by doc
plot_wordCount_Type <- summary(legalCorpus)

ggplot(data = plot_wordCount_Type, aes(x = Type, y = Tokens, group = 1)) +
geom_line() +
geom_point() +
theme_bw()

# top word freqs by doc (bin sets the # words)
ggplot(legalFreqs_bydoc2, aes(n/total, fill = DocName)) +
  geom_histogram(show.legend = FALSE, bins = 50) +
  facet_wrap(~DocName, ncol = 2, scales = "free_y")


# let's plot the Zipf ranks w/ a log scale (see if rank & freq are related)
legalZipf %>%
  ggplot(aes(rank, `term_freq`, color = DocName)) +
  geom_line(size = 0.9, alpha = 0.6, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()

# Power Law. Let's check the middle section & plot
legalSubset <- legalZipf %>%
  filter(rank < 500, rank > 10)

lm(log10(`term_freq`) ~ log10(rank), data = legalSubset)

legalZipf %>%
  ggplot(aes(rank, `term_freq`, color = DocName)) +
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 0.9, alpha = 0.6, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()


# words with a high tf-idf score
legal_tfidf_high %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(DocName) %>%
  top_n(1) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = DocName)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~DocName, ncol = 1, scales = "free") +
  coord_flip()

