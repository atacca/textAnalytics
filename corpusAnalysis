##### ########################################################
#####               CORPUS ANALYSIS                     ######
#####         - it's a work in progress -               ######
##### The goal is to have all the code ready to go.     ######
##### You'll see the variable naming is specific,       ######
##### that's in case if you want to do a corpus-corpus  ######
##### comparison, you won't find that all your global   ######
##### variables are named the same. When changing to    ######
##### a new corpus, just run a find-replace on the      ######
##### specific name (e.g. "legal-") to the new one.     ######
##### ########################################################

library(tidyverse)
library(tidytext)
library(readtext)
library(magrittr)
library(broom)
library(tokenizers)
library(quanteda)
# library(data.table)
library(udpipe)
library(pdftools)
library(lattice)
library(igraph)
library(ggraph)

##### ######################################
##### Let's do the pre-lim stuff..
##### ######################################

### set the location of the source files for the corpus
legalSourcesLocation <- "sources/"

# and a list of the individual files
legalSourcesList <- list.files(path = legalSourcesLocation)

### grab the texts with quanteda
legalTexts <- readtext(legalSourcesLocation,
                        docvarsfrom = "filenames",
                        docvarnames = c("Institution", "DocName",
                          "Type", "Country", "Year"),
                        dvsep = "_")

### this is the default stopwords list
default_stopwords <- unnest_tokens(read.csv("default_stopwords.csv",
                      stringsAsFactors = FALSE), word, word) %>%
                      as_tibble()

### ...and our custom stopwords list...
legalStopwords <- unnest_tokens(read.csv("legalEnglish_stopwords.csv",
                    stringsAsFactors = FALSE), word, word) %>%
                    as_tibble()

# ...and combine those with default stopwords...
stopwords_all <- bind_rows(default_stopwords, legalStopwords)


##### #############################
##### OK, IT'S CORPUS MAKING TIME
##### #############################

# 1st is the corpus in tidy format, with no words removed
legalTidy_allWords <- legalTexts %>%
  group_by(DocName) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)  # word is the name of the output column

# 2nd is the corpus in tidy format, with custom words removed
legalTidy_noCustom <- legalTidy_allWords %>%
  anti_join(legalStopwords, by = "word")

# 3rd is the corpus in tidy format, with all stops removed (custom + general)
legalTidy_noStops <- legalTidy_allWords %>%
  anti_join(stopwords_all, by = "word")

# a quanteda corpus (all words)
legalCorpus_quanteda <- corpus(legalTexts)


##### ###########################
##### COUNTS & WORD FREQUENCIES
##### ###########################

## Let's start with a count of the documents in the corpus
legalDocCount <- legalTexts %>%
  count(DocName) %>%
  summarize(total = sum(n))

## then get the top word freqs (stopwords removed) & make it a DF so we can choose how many rows
legalFreqs <- count(legalTidy_noStops, word, sort = TRUE) %>%
                ungroup() %>%
                data.frame()

# and with just the custom words removed
legalFreqs_noCustom <- count(legalTidy_noCustom, word, sort = TRUE) %>%
                          ungroup() %>%
                          data.frame()

# and a freq count of words by document ##### CLEAN THIS UP WITH PIPING #####
legalFreqs_bydoc <- legalTidy_noStops %>%
  count(DocName, word, sort = TRUE)

legalWordCountByDoc <- legalFreqs_bydoc %>%
                          group_by(DocName) %>%
                          summarize(Doc_totalWords = sum(n))

legalFreqs_bydoc2 <- left_join(legalFreqs_bydoc, legalWordCountByDoc)


##### ###############
##### BIGRAMS, BABY
##### ###############

## let's tokenise the entire corpus into bigrams (all words)
legalBigrams_all <- legalTexts %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# ok, how about the most common bigrams?
legalBigrams_count <- legalBigrams_all %>%
  count(bigram, sort = TRUE) %>%
  data.frame()

### now we'll put the words into separate columns, filter, then paste the bigrams together
legalBigrams_sep <- legalBigrams_all %>%
  separate(bigram, c("word1", "word2"), sep = " ")

legalBigrams_filtered <- legalBigrams_sep %>%
  filter(!word1 %in% legalStopwords$word) %>%
  filter(!word2 %in% legalStopwords$word)

legalBigrams_filtered2 <- paste(legalBigrams_filtered$word1,
                                legalBigrams_filtered$word2, sep = " ") %>%
                                data.frame()

colnames(legalBigrams_filtered2) <- paste("bigrams")

legalBigrams_filtered2 <- legalBigrams_filtered2 %>%
                                count(bigrams, sort = TRUE) %>%
                                data.frame()


##### ##################
##### IT's TRIGRAM TIME
##### ##################

## same as bigrams, let's tokenise the entire corpus into trigrams (no stops)
legalTrigrams_all <- legalTexts %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# let's have a look-see at the most common trigrams then
legalTrigrams_count <- legalTrigrams_all %>%
  count(trigram, sort = TRUE) %>%
  data.frame()

### now we'll put the words into separate columns, filter, then paste the trigrams together
legalTrigrams_sep <- legalTrigrams_all %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

legalTrigrams_filtered <- legalTrigrams_sep %>%
  filter(!word1 %in% legalStopwords$word) %>%
  filter(!word2 %in% legalStopwords$word) %>%
  filter(!word3 %in% legalStopwords$word)

legalTrigrams_filtered2 <- paste(legalTrigrams_filtered$word1,
                                legalTrigrams_filtered$word2,
                                legalTrigrams_filtered$word3,
                                sep = " ") %>%
                              data.frame()

colnames(legalTrigrams_filtered2) <- paste("trigrams")

legalTrigrams_filtered2 <- legalTrigrams_filtered2 %>%
                            count(trigrams, sort = TRUE) %>%
                            data.frame()


##### #####################
##### TF-IDF...IDK EITHER
##### #####################

### Let's calculate term frequency rankings (Zipf's Law)
legalZipf <- legalFreqs_bydoc2 %>%
  mutate(rank = row_number(), `term_freq` = n/Doc_totalWords) %>%
  arrange(desc(term_freq)) %>%
  data.frame()

# let's start with the tf-idf for all unigrams
legal_tfidf <- legalFreqs_bydoc2 %>%
  bind_tf_idf(word, DocName, n)

# and look at the unigrams with a high score & trim the document names
legal_tfidf_high <- legal_tfidf %>%
  select(-Doc_totalWords) %>%
  arrange(desc(tf_idf)) %>%
  data.frame() 
legal_tfidf_high$DocName <- substr(legal_tfidf_high$DocName, 1, 30)

# moving on to bigrams now. Let's start with the top ones (1st unite the bigrams back together)
legalBigrams_tfidf <- legalBigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(DocName, bigram) %>%
  bind_tf_idf(bigram, DocName, n) %>%
  arrange(desc(tf_idf)) %>%
  data.frame()

# ok, now the top trigrams
legalTrigrams_tfidf <- legalTrigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ") %>%
  count(DocName, trigram) %>%
  bind_tf_idf(trigram, DocName, n) %>%
  arrange(desc(tf_idf)) %>%
  data.frame()


##### ###########################################
##### DID SOMEONE SAY POS? that's just rude..
##### ###########################################

### download the English model for udpipe (only needs to be done once - remove #s if needed)
# dl_udpipe_engModel <- udpipe_download_model(language = "english")
# dl_udpipe_engModel
# str(dl_udpipe_engModel)

## now set a variable for the english model (gonna use for POS tagging and whatnot)
udmodel_eng <- udpipe_load_model(file = "english-ud-2.0-170801.udpipe")

### loop through each file and bind in a df
#### doing it this way because having a lot of trouble doing it other ways (kept crashing the mac)
##### at the moment this is only looking at PDF files. will need to expand this to other types.
legal_annotated_list <- list()
i = 1
for (i in 1:length(legalSourcesList)) {
  legalFile <- c(paste(legalSourcesLocation, legalSourcesList[i], sep = ""))
  txt <- pdf_text(legalFile)
  x = udpipe_annotate(udmodel_eng, x = txt)
  x = as.data.frame(x)
  legal_annotated_list[[i]] <- x
  i <- i + 1
}
legalAnnotated_pos <- do.call(rbind, legal_annotated_list)

### Now we can get lists of different POS. Imma try to loop through this...
POS_words <- c("VERB", "ADV", "NOUN", "PROPN", "ADJ")
POS_topWords_list <- list() 

for (word in POS_words) {
  x = subset(legalAnnotated_pos, upos %in% word)
  x = txt_freq(x$token)
  x$key = factor(x$key, levels = rev(x$key))
  x = head(x, 20)
  POS_topWords_list[[word]] <- x
}
legalPOS_topWords <- do.call(rbind, POS_topWords_list)


##### ###############################
##### KWIC - TFIDF, FREQUENT WORDS
##### ###############################

#### loop through kwic based on highest tf-idf tokens, output to list then bind
legal_kwic_list_tfidf <- list()
i = 1
while (i <= 10) {
  kwic_results <- kwic(legalCorpus_quanteda, legal_tfidf_high$word[i], window = 10,
    valuetype = "glob", case_insensitive = TRUE)
  kwic_results$i <- i
  
  legal_kwic_list_tfidf[[i]] <- kwic_results
  i <- i +1
}
legal_kwic_tfidf <- do.call(rbind, legal_kwic_list_tfidf)

# now clean up the results for printing by combining the kwic sentences & trim the df...
legal_kwic_tfidfOutput <- data.frame(legal_kwic_tfidf)

legal_kwic_tfidfOutput$sentences <- paste(legal_kwic_tfidf$pre,
                                              " | ",
                                              legal_kwic_tfidf$keyword,
                                              " | ",
                                              legal_kwic_tfidf$post)

legal_kwic_tfidfOutput$docname <- substr(legal_kwic_tfidfOutput$docname, 1, 20)

legal_kwic_tfidfOutput <- legal_kwic_tfidfOutput[, -c(2:4, 6:7)]


#### loop through kwic based on the top 10 word freqs (all stops removed)
legal_kwic_list_freqs <- list()
i = 1
while (i <=10) {
  kwic_freqs <- kwic(legalCorpus_quanteda, legalFreqs$word[i], window = 10,
    valuetype = "glob", case_insensitive = TRUE)
  kwic_freqs$i <- i
  
  legal_kwic_list_freqs[[i]] <- kwic_freqs
  i = i + 1
}
legal_kwic_freqs <- do.call(rbind, legal_kwic_list_freqs)

# clean up the kwic results & trim the df...
legal_kwic_freqsOutput <- data.frame(legal_kwic_freqs)

legal_kwic_freqsOutput$sentences <- paste(legal_kwic_freqs$pre,
                                              " | ",
                                              legal_kwic_freqs$keyword,
                                              " | ",
                                              legal_kwic_freqs$post)

legal_kwic_freqsOutput$docname <- substr(legal_kwic_freqsOutput$docname, 1, 20)

legal_kwic_freqsOutput <- legal_kwic_freqsOutput[, -c(2:4, 6:7)]



##### #############################################################################
##### OUTPUTS TO FILE - 4 FILES (Basic Stats, Top Ngrams, KWIC_tfidf, KWIC_freqs)
##### #############################################################################

### 1: Basic Statistics ###
legalOutputs_stats <- file("outputs/legalEnglishCorpus_statistics.txt", open = "w")

sink(file = legalOutputs_stats, type = "output")
cat("Basic Statistics for the Legal English Corpus\n\n\n")
cat("Document Count\n")
legalDocCount
cat("\n\n\n", "Word Count for the Corpus - no word filtering\n",
  "this is the complete word count, and includes abbreviations, symbols, etc\n\n")
count(legalTidy_allWords)
cat("\n\n\n", "Word Count - custom stop words removed\n",
  "unhelpful words are removed, such as abbreviations, symbols, country names, etc\n\n")
count(legalTidy_noCustom)
cat("\n\n\n", "Word Count - custom and common words removed\n\n",
  "common words are also removed, such as articles, prepositions, and pronouns\n\n")
count(legalTidy_noStops)
cat("\n\n\n", "Sum of custom and common words that can be removed\n\n")
count(legalTidy_allWords) - count(legalTidy_noStops)
cat("\n\n\n", "NOTE: Analyses are done with either all stop words removed (custom & common), or\n
  with only the custom removed, depending on what the end goal is. E.g. Top 50 frequencies\n
  have both removed, because we want to see the most important words for the corpus/industry,\n
  while top bigrams, trigrams, and longer language only have custom words removed, because\n
  the 'common' words can play an integral role in those.\n\n\n")
cat("\n\n\n", "Part of Speech counts for the corpus\n
    for a list of what these mean, see: http://universaldependencies.org/u/pos/index.html\n\n")
txt_freq(legalAnnotated_pos$upos)
sink()

### 2: Top Ngrams ###
legalOutputs_ngrams <- file("outputs/legalEnglishCorpus_topNgrams.txt", open = "w")

sink(file = legalOutputs_ngrams, type = "output")
cat("\n\n\n", "Top 20 Words for different Parts of Speech\n
  (the categories shown here, and the # of each can be changed as desired\n\n")
legalPOS_topWords
cat("\n\n\n", "Top 50 Word Frequencies for the corpus (stop words removed)\n\n")
legalFreqs[1:50,]
cat("\n\n\n", "For Comparison: Top 50 Word Frequencies (common words included)\n\n")
legalFreqs_noCustom[1:50,]
cat("\n\n\n", "Top Bigrams (no filtering)\n\n")
legalBigrams_count[1:50,]
cat("\n\n\n", "Top Trigrams (no filtering)\n\n")
legalTrigrams_count[1:50,]
cat("\n\n\n", "Top 50 Highest Scoring Words by tf-idf\n\n
  In the below table:\n
  \t - \t'n'\t is the number of times the word appears in the listed document.\n
  \t - \t'tf'\t is the term frequency score, giving a relative weighting within the document.\n
  \t - \t'idf'\t is the inverse document freq. Relates to how common a word is in a corpus.\n
  \t - \t'tf-idf'\t combines them. High tf with a low idf means the word is more specific.\n\n")
legal_tfidf_high[1:50,]
cat("\n\n\n", "Top 30 Bigrams by tf-idf\n\n")
head(legalBigrams_tfidf, 30)
cat("\n\n\n", "Top 30 Trigrams by tf-idf\n\n")
head(legalTrigrams_tfidf, 30)
sink()

### 3: Top KWIC (tfidf) sentences ###
legalOutputs_kwic <- file("outputs/legalEnglishCorpus_KWICtfidf.txt", open = "w")

write.table(legal_kwic_tfidfOutput, file = legalOutputs_kwic, col.names = FALSE)

### 4: Top KWIC (most frequent words) sentences ###
legalOutputs_kwic <- file("outputs/legalEnglishCorpus_KWICfreqs.txt", open = "w")

write.table(legal_kwic_freqsOutput, file = legalOutputs_kwic, col.names = FALSE)

