##### ########################################################
#####               CORPUS ANALYSIS                     ######
#####         - it's a work in progress -               ######
##### The goal is to have all the code ready to go.     ######
##### You'll see the variable naming is specific,       ######
##### that's in case if you want to do a corpus-corpus  ######
##### comparison, you won't find that all your global   ######
##### variables are named the same. When changing to    ######
##### a new corpus, just run a find-replace on the      ######
##### specific name (e.g. "legal-") to the new one.     ######
##### ########################################################

library(tidyverse)
library(tidytext)
library(readtext)
library(magrittr)
library(broom)
library(tokenizers)
library(quanteda)
library(udpipe)
library(pdftools)
library(lattice)
library(igraph)
library(ggraph)
library(openxlsx)
library(reshape2)

##### ######################################
##### Let's do the pre-lim stuff..
##### ######################################

### set the location of the source files for the corpus
legalSourcesLocation <- "sources/"

# and a list of the individual files
legalSourcesList <- list.files(path = legalSourcesLocation)

### grab the texts with quanteda
legalTexts <- readtext(legalSourcesLocation,
                        docvarsfrom = "filenames",
                        docvarnames = c("Institution", "DocName",
                          "Type", "Country", "Year"),
                        dvsep = "_")

### this is the default stopwords list
default_stopwords <- unnest_tokens(read.csv("default_stopwords.csv",
                      stringsAsFactors = FALSE), word, word) %>%
                      as_tibble()

### ...and our custom stopwords list...
legalStopwords <- unnest_tokens(read.csv("legalEnglish_stopwords.csv",
                    stringsAsFactors = FALSE), word, word) %>%
                    as_tibble()

# ...and combine those with default stopwords...
stopwords_all <- bind_rows(default_stopwords, legalStopwords)


##### #############################
##### OK, IT'S CORPUS MAKING TIME
##### #############################

# 1st is the corpus in tidy format, with no words removed
legalTidy_allWords <- legalTexts %>%
  group_by(DocName) %>%
  mutate(linenumber = row_number()) %>%
  ungroup() %>%
  unnest_tokens(word, text)  # word is the name of the output column

#2nd is the corpus in tidy format, with the default stops removed
legalTidy_noDefaultStops <- legalTidy_allWords %>%
  anti_join(default_stopwords, by = "word")

# 3rd is the corpus in tidy format, with custom words removed
legalTidy_noCustomStops <- legalTidy_allWords %>%
  anti_join(legalStopwords, by = "word")

# 4th is the corpus in tidy format, with all stops removed (custom + general)
legalTidy_noStops <- legalTidy_allWords %>%
  anti_join(stopwords_all, by = "word")

# 5th is a quanteda corpus (all words)
legalCorpus_quanteda <- corpus(legalTexts)


##### #########
##### COUNTS
##### #########

## document count
legalDocCount <- legalTexts %>%
  count(DocName) %>%
  summarize(total = sum(n))
colnames(legalDocCount) <- paste("Document Count")

## corpus word count - all words
legalCorpusCount_allWords <- count(legalTidy_allWords) %>%
  data.frame()
colnames(legalCorpusCount_allWords) <- paste("Word Count - all words")

## corpus word count - all stops removed (default + custom)
legalCorpusCount_noStops <- count(legalTidy_noStops) %>%
  data.frame()
colnames(legalCorpusCount_noStops) <- paste("Word Count - all stopwords removed")

## corpus word count - default stops removed
legalCorpusCount_noDefaultStops <- count(legalTidy_noDefaultStops) %>%
  data.frame()
colnames(legalCorpusCount_noDefaultStops) <- paste("Word Count - only default stops removed")

## corpus word count - custom stops removed
legalCorpusCount_noCustomStops <- count(legalTidy_noCustomStops) %>%
  data.frame()
colnames(legalCorpusCount_noCustomStops) <- paste("Word Count - only custom stops removed")

## sum of default stops plus custom stops
legalCorpusCount_sumAllStops <- count(legalTidy_allWords) - count(legalTidy_noStops) %>%
  data.frame()
colnames(legalCorpusCount_sumAllStops) <- paste("Sum of default and custom stopwords")


##### ###########################
##### WORD FREQUENCIES
##### ###########################


## let's get the top word freqs (stops removed) & make it a DF so we can choose how many rows
legalFreqs <- count(legalTidy_noStops, word, sort = TRUE) %>%
                ungroup() %>%
                data.frame()

# we'll do a version with the default words removed. This will help choose the custom stops
legalFreqs_noDefault <- count(legalTidy_noDefaultStops, word, sort = TRUE) %>%
                          ungroup() %>%
                          data.frame()

# and with just the custom words removed
legalFreqs_noCustom <- count(legalTidy_noCustomStops, word, sort = TRUE) %>%
                          ungroup() %>%
                          data.frame()

# and a freq count of words by document ##### CLEAN THIS UP WITH PIPING #####
legalFreqs_bydoc <- legalTidy_noStops %>%
  count(DocName, word, sort = TRUE)

legalWordCountByDoc <- legalFreqs_bydoc %>%
                          group_by(DocName) %>%
                          summarize(Doc_totalWords = sum(n))

legalFreqs_bydoc2 <- left_join(legalFreqs_bydoc, legalWordCountByDoc)


##### ###############
##### BIGRAMS, BABY
##### ###############

## let's tokenise the entire corpus into bigrams (all words)
legalBigrams_all <- legalTexts %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

# ok, how about the most common bigrams?
legalBigrams_count <- legalBigrams_all %>%
  count(bigram, sort = TRUE) %>%
  data.frame()

### now we'll put the words into separate columns, filter, then paste the bigrams together
legalBigrams_sep <- legalBigrams_all %>%
  separate(bigram, c("word1", "word2"), sep = " ")

legalBigrams_filtered <- legalBigrams_sep %>%
  filter(!word1 %in% legalStopwords$word) %>%
  filter(!word2 %in% legalStopwords$word)

legalBigrams_filtered2 <- paste(legalBigrams_filtered$word1,
                                legalBigrams_filtered$word2, sep = " ") %>%
                                data.frame()

colnames(legalBigrams_filtered2) <- paste("bigrams")

legalBigrams_filtered2 <- legalBigrams_filtered2 %>%
                                count(bigrams, sort = TRUE) %>%
                                data.frame()


##### ##################
##### IT's TRIGRAM TIME
##### ##################

## same as bigrams, let's tokenise the entire corpus into trigrams (no stops)
legalTrigrams_all <- legalTexts %>%
  unnest_tokens(trigram, text, token = "ngrams", n = 3)

# let's have a look-see at the most common trigrams then
legalTrigrams_count <- legalTrigrams_all %>%
  count(trigram, sort = TRUE) %>%
  data.frame()

### now we'll put the words into separate columns, filter, then paste the trigrams together
legalTrigrams_sep <- legalTrigrams_all %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

legalTrigrams_filtered <- legalTrigrams_sep %>%
  filter(!word1 %in% legalStopwords$word) %>%
  filter(!word2 %in% legalStopwords$word) %>%
  filter(!word3 %in% legalStopwords$word)

legalTrigrams_filtered2 <- paste(legalTrigrams_filtered$word1,
                                legalTrigrams_filtered$word2,
                                legalTrigrams_filtered$word3,
                                sep = " ") %>%
                              data.frame()

colnames(legalTrigrams_filtered2) <- paste("trigrams")

legalTrigrams_filtered2 <- legalTrigrams_filtered2 %>%
                            count(trigrams, sort = TRUE) %>%
                            data.frame()


##### #####################
##### TF-IDF...IDK EITHER
##### #####################

### Let's calculate term frequency rankings (Zipf's Law)
legalZipf <- legalFreqs_bydoc2 %>%
  mutate(rank = row_number(), `term_freq` = n/Doc_totalWords) %>%
  arrange(desc(term_freq)) %>%
  data.frame()

# let's start with the tf-idf for all unigrams
legal_tfidf <- legalFreqs_bydoc2 %>%
  bind_tf_idf(word, DocName, n)

# and look at the unigrams with a high score & trim the document names
legal_tfidf_high <- legal_tfidf %>%
  select(-Doc_totalWords) %>%
  arrange(desc(tf_idf)) %>%
  data.frame() 
legal_tfidf_high$DocName <- substr(legal_tfidf_high$DocName, 1, 30)

# moving on to bigrams now. Let's start with the top ones (1st unite the bigrams back together)
legalBigrams_tfidf <- legalBigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(DocName, bigram) %>%
  bind_tf_idf(bigram, DocName, n) %>%
  arrange(desc(tf_idf)) %>%
  data.frame()

# ok, now the top trigrams
legalTrigrams_tfidf <- legalTrigrams_filtered %>%
  unite(trigram, word1, word2, word3, sep = " ") %>%
  count(DocName, trigram) %>%
  bind_tf_idf(trigram, DocName, n) %>%
  arrange(desc(tf_idf)) %>%
  data.frame()


##### ###########################################
##### DID SOMEONE SAY POS? that's just rude..
##### ###########################################

### download the English model for udpipe (only needs to be done once - remove #s if needed)
# dl_udpipe_engModel <- udpipe_download_model(language = "english")
# dl_udpipe_engModel
# str(dl_udpipe_engModel)

## now set a variable for the english model (gonna use for POS tagging and whatnot)
udmodel_eng <- udpipe_load_model(file = "english-ud-2.0-170801.udpipe")

### loop through each file and bind in a df
#### doing it this way because having a lot of trouble doing it other ways (kept crashing the mac)
##### at the moment this is only looking at PDF files. will need to expand this to other types.
legal_annotated_list <- list()
i = 1
for (i in 1:length(legalSourcesList)) {
  legalFile <- c(paste(legalSourcesLocation, legalSourcesList[i], sep = ""))
  txt <- pdf_text(legalFile)
  x = udpipe_annotate(udmodel_eng, x = txt)
  x = as.data.frame(x)
  legal_annotated_list[[i]] <- x
  i <- i + 1
}
legalAnnotated_pos <- do.call(rbind, legal_annotated_list)

### Now we can get lists of different POS. Imma try to loop through this...
POS_words <- c("NOUN", "PROPN", "PRON", "ADJ", "VERB", "ADV", "AUX",
                "ADP", "DET", "CCONJ", "SCONJ", "INTJ")
POS_topWords_list <- list() 

for (word in POS_words) {
  x = subset(legalAnnotated_pos, upos %in% word)
  x = txt_freq(x$token)
  x$key = factor(x$key, levels = rev(x$key))
  x = head(x, 20)
  POS_topWords_list[[word]] <- x
}
legalPOS_topWords <- do.call(rbind, POS_topWords_list)


##### ###############################
##### KWIC - TFIDF, FREQUENT WORDS
##### ###############################

#### loop through kwic based on highest tf-idf tokens, output to list then bind
legal_kwic_list_tfidf <- list()
i = 1
while (i <= 10) {
  kwic_results <- kwic(legalCorpus_quanteda, legal_tfidf_high$word[i], window = 10,
    valuetype = "glob", case_insensitive = TRUE)
  kwic_results$i <- i
  
  legal_kwic_list_tfidf[[i]] <- kwic_results
  i <- i +1
}
legal_kwic_tfidf <- do.call(rbind, legal_kwic_list_tfidf)

# now clean up the results for printing by combining the kwic sentences & trim the df...
legal_kwic_tfidfOutput <- data.frame(legal_kwic_tfidf)

legal_kwic_tfidfOutput$sentences <- paste(legal_kwic_tfidf$pre,
                                              " | ",
                                              legal_kwic_tfidf$keyword,
                                              " | ",
                                              legal_kwic_tfidf$post)

legal_kwic_tfidfOutput$docname <- substr(legal_kwic_tfidfOutput$docname, 1, 20)

legal_kwic_tfidfOutput <- legal_kwic_tfidfOutput[, -c(2:4, 6:7)]


#### loop through kwic based on the top 10 word freqs (all stops removed)
legal_kwic_list_freqs <- list()
i = 1
while (i <=10) {
  kwic_freqs <- kwic(legalCorpus_quanteda, legalFreqs$word[i], window = 10,
    valuetype = "glob", case_insensitive = TRUE)
  kwic_freqs$i <- i
  
  legal_kwic_list_freqs[[i]] <- kwic_freqs
  i = i + 1
}
legal_kwic_freqs <- do.call(rbind, legal_kwic_list_freqs)

# clean up the kwic results & trim the df...
legal_kwic_freqsOutput <- data.frame(legal_kwic_freqs)

legal_kwic_freqsOutput$sentences <- paste(legal_kwic_freqs$pre,
                                              " | ",
                                              legal_kwic_freqs$keyword,
                                              " | ",
                                              legal_kwic_freqs$post)

legal_kwic_freqsOutput$docname <- substr(legal_kwic_freqsOutput$docname, 1, 20)

legal_kwic_freqsOutput <- legal_kwic_freqsOutput[, -c(2:4, 6:7)]



##### #################################################
##### OUTPUT TO SPREADSHEET (using openxlsx)
##### 1st, create any vars that aren't already above,
##### then write each one to the spreadsheet
##### #################################################

## COUNTS - Make a combined df of the counts from the COUNT section
legalCorpusCounts_CombinedCounts <- cbind(legalDocCount,
                                          legalCorpusCount_allWords,
                                          legalCorpusCount_noStops,
                                          legalCorpusCount_noDefaultStops,
                                          legalCorpusCount_noCustomStops,
                                          legalCorpusCount_sumAllStops) %>%
                                    melt(variable.name = "Object",
                                      value.name = "Count")

## COUNTS - this is a count of the different POSs
legalPOSCounts <- txt_freq(legalAnnotated_pos$upos)


### CREATE THE WORKBOOK ###
legal_wb <- createWorkbook()

### create a worksheet for each data element and populate
addWorksheet(legal_wb, "doc & word counts") 
writeData(legal_wb, "doc & word counts", legalCorpusCounts_CombinedCounts) 

addWorksheet(legal_wb, "POS counts")
writeData(legal_wb, "POS counts", legalPOSCounts)

addWorksheet(legal_wb, "Top 20 POSs")
writeData(legal_wb, "Top 20 POSs", legalPOS_topWords, rowNames = TRUE)

addWorksheet(legal_wb, "Top 200 words")
writeData(legal_wb, "Top 200 words", legalFreqs[1:200,])

addWorksheet(legal_wb, "Top 75 bigrams")
writeData(legal_wb, "Top 75 bigrams", legalBigrams_count[1:75,])

addWorksheet(legal_wb, "Top 75 trigrams")
writeData(legal_wb, "Top 75 trigrams", legalTrigrams_count[1:75,])

addWorksheet(legal_wb, "KWIC - Top 20 words")
writeData(legal_wb, "KWIC - Top 20 words", legal_kwic_freqsOutput)

addWorksheet(legal_wb, "KWIC - Top tf-idf words")
writeData(legal_wb, "KWIC - Top tf-idf words", legal_kwic_tfidfOutput)

addWorksheet(legal_wb, "Top 100 tf-idf words")
writeData(legal_wb, "Top 100 tf-idf words", legal_tfidf_high[1:100,])

addWorksheet(legal_wb, "Top 75 tf-idf bigrams")
writeData(legal_wb, "Top 75 tf-idf bigrams", legalBigrams_tfidf[1:75,])

addWorksheet(legal_wb, "Top 75 tf-idf trigrams")
writeData(legal_wb, "Top 75 tf-idf trigrams", legalTrigrams_tfidf[1:75,])

## finally, save the workbook
saveWorkbook(legal_wb, file = "outputs/legalOututs.xlsx", overwrite = TRUE)
